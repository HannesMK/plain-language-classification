{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition and Evaluation\n",
    "## Table of Contents\n",
    "1. [Model Selection](#model-selection)\n",
    "2. [Feature Engineering](#feature-engineering)\n",
    "3. [Hyperparameter Tuning](#hyperparameter-tuning)\n",
    "4. [Implementation](#implementation)\n",
    "5. [Evaluation Metrics](#evaluation-metrics)\n",
    "6. [Comparative Analysis](#comparative-analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tf_keras\n",
    "import transformers\n",
    "\n",
    "# Add the parent directory to the path so that the custom utilities module is found\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "import utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "The first step was to try an LSTM model to see if the preserving of sequence improves model performance. The LSTM model performed similar to the baseline model. Next, I tried using pre-trained embeddings to see if leveraging models trained on larger datasets aids performance. However, model performance did not increase, but even declined when using pre-trained embeddings from a Neural-Net Language Model. Lastly, a more advanced Transformer-based model was fine tuned to the project task, which finally yielded a substantial improvement over baseline performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "No additional feature engineering was done compared to the data preparation already implemented for the baseline model. However, for the Transformer-based model, stop words were left in, and larger sequence lengths were used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_duplicate_removal = True\n",
    "has_stop_word_removal = True\n",
    "validation_size = 0.1\n",
    "test_size = 0.2\n",
    "\n",
    "x_training, x_validation, x_test, y_training, y_validation, y_test = (\n",
    "    utilities.prepare_and_split_data(\n",
    "        has_duplicate_removal=has_duplicate_removal,\n",
    "        has_stop_word_removal=has_stop_word_removal,\n",
    "        validation_size=validation_size,\n",
    "        test_size=test_size,\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(x_training)}\")\n",
    "print(f\"Validation set size: {len(x_validation)}\")\n",
    "print(f\"Test set size: {len(x_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "Hyperparameters were tuned manually by trying different parameters such as layer size and learning rates and seeing what works best. In general, model performance was similar for varying parameter values. Naturally, automated hyperparameter tuning would be preferable over manual tuning, as this could systematically cover a wider range of possible values and objectively find an optimal solution. Such an automated approach using Grid Search or Random Search may be implemented in future versions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "As stated above, three different models were implemented and compared to the baseline model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long Short-Term Memory Model\n",
    "\n",
    "The first model is a bidirectional LSTM-model, which preservence information about the sequence of the words/tokens. As with the baseline model, embeddings are trained from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 10_000\n",
    "maximum_sequence_length = 12\n",
    "standardization_method = \"lower\"\n",
    "dimensions_embedding = 32\n",
    "units_lstm_1 = 64\n",
    "units_lstm_2 = 32\n",
    "units_lstm_3 = 8\n",
    "units_dense = 8\n",
    "\n",
    "model_lstm = utilities.build_model_lstm(\n",
    "    x_training=x_training,\n",
    "    vocabulary_size=vocabulary_size,\n",
    "    maximum_sequence_length=maximum_sequence_length,\n",
    "    standardization_method=standardization_method,\n",
    "    dimensions_embedding=dimensions_embedding,\n",
    "    units_lstm_1=units_lstm_1,\n",
    "    units_lstm_2=units_lstm_2,\n",
    "    units_lstm_3=units_lstm_3,\n",
    "    units_dense=units_dense,\n",
    ")\n",
    "\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_epochs = 5\n",
    "batch_size = 512\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.BinaryCrossentropy()\n",
    "metrics = [tf.keras.metrics.BinaryAccuracy()]\n",
    "\n",
    "model_lstm.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "history_lstm = model_lstm.fit(\n",
    "    x_training,\n",
    "    y_training,\n",
    "    epochs=number_of_epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(x_validation, y_validation),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_limits = (0.75, 1)\n",
    "\n",
    "figure, axes = utilities.plot_accuracy(history=history_lstm, y_limits=y_limits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, axes = utilities.plot_loss(history=history_lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model With Pre-Trained Embeddings From a Neural-Net Language Model\n",
    "\n",
    "The second model is a structurally similar to the baseline model, but instead of training embeddings from scratch, pre-trained embeddings from an NNLM are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dimensions = 128  # can be 50 or 128\n",
    "units_dense = 16\n",
    "is_trainable = True\n",
    "\n",
    "model_nnlm = utilities.build_model_nnlm(\n",
    "    embedding_dimensions=embedding_dimensions,\n",
    "    units_dense=units_dense,\n",
    ")\n",
    "\n",
    "model_nnlm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_epochs = 50\n",
    "batch_size = 512\n",
    "\n",
    "initial_learning_rate = 5e-5\n",
    "\n",
    "# tf_keras is used instead of tf.keras, because tensorflow_hub (Version 0.16.1) is incompatible with Keras 3\n",
    "optimizer = tf_keras.optimizers.Adam(learning_rate=initial_learning_rate)\n",
    "loss = tf_keras.losses.BinaryCrossentropy()\n",
    "metrics = [tf_keras.metrics.BinaryAccuracy()]\n",
    "\n",
    "model_nnlm.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss,\n",
    "    metrics=metrics,\n",
    ")\n",
    "\n",
    "history_nnlm = model_nnlm.fit(\n",
    "    x_training,\n",
    "    y_training,\n",
    "    epochs=number_of_epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(x_validation, y_validation),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_limits = (0.5, 1)\n",
    "x_ticks = np.arange(start=5, stop=51, step=5).tolist()\n",
    "\n",
    "figure, axes = utilities.plot_accuracy(history=history_nnlm, y_limits=y_limits, x_ticks=x_ticks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, axes = utilities.plot_loss(history=history_nnlm, x_ticks=x_ticks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuned German BERT\n",
    "\n",
    "The third and final model is a fine-tuned version of the Transformer-based German BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_duplicate_removal = True\n",
    "has_stop_word_removal = False\n",
    "\n",
    "x_training, x_validation, x_test, y_training, y_validation, y_test = (\n",
    "    utilities.prepare_and_split_data(\n",
    "        has_duplicate_removal=has_duplicate_removal,\n",
    "        has_stop_word_removal=has_stop_word_removal,\n",
    "        validation_size=validation_size,\n",
    "        test_size=test_size,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_gbert = \"deepset/gbert-base\"\n",
    "maximum_sequence_length = 30\n",
    "\n",
    "data_training = utilities.tokenize_text_gbert(\n",
    "    texts=x_training.tolist(),\n",
    "    labels=y_training.tolist(),\n",
    "    maximum_sequence_length=maximum_sequence_length,\n",
    "    model_name=model_name_gbert,\n",
    ")\n",
    "\n",
    "data_validation = utilities.tokenize_text_gbert(\n",
    "    texts=x_validation.tolist(),\n",
    "    labels=y_validation.tolist(),\n",
    "    maximum_sequence_length=maximum_sequence_length,\n",
    "    model_name=model_name_gbert,\n",
    ")\n",
    "\n",
    "data_test = utilities.tokenize_text_gbert(\n",
    "    texts=x_test.tolist(),\n",
    "    labels=y_test.tolist(),\n",
    "    maximum_sequence_length=maximum_sequence_length,\n",
    "    model_name=model_name_gbert,\n",
    ")\n",
    "\n",
    "model_gbert = transformers.TFBertForSequenceClassification.from_pretrained(\n",
    "    model_name_gbert,\n",
    "    num_labels=1,\n",
    ")\n",
    "\n",
    "model_gbert.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "batch_size = 64\n",
    "initial_learning_rate = 5e-5\n",
    "\n",
    "# tf_keras is used instead of tf.keras, because transformers (Version 4.43.3) is incompatible with Keras 3\n",
    "optimizer = tf_keras.optimizers.Adam(learning_rate=initial_learning_rate)\n",
    "loss = tf_keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "metrics = [tf_keras.metrics.BinaryAccuracy()]\n",
    "\n",
    "model_gbert.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "history_gbert = model_gbert.fit(\n",
    "    data_training.shuffle(1000).batch(batch_size),\n",
    "    validation_data=data_validation.batch(batch_size),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_limits = (0.9, 1)\n",
    "\n",
    "figure, axes = utilities.plot_accuracy(history=history_gbert, y_limits=y_limits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, axes = utilities.plot_loss(history=history_gbert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "As with the baseline model, accuracy was chosen as the evaluation metric, since the two classes are well-balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_lstm = utilities.evaluate_model(\n",
    "    model=model_lstm,\n",
    "    x_test=x_test,\n",
    "    y_test=y_test,\n",
    ")\n",
    "\n",
    "accuracy_nnlm = utilities.evaluate_model(\n",
    "    model=model_nnlm,\n",
    "    x_test=x_test,\n",
    "    y_test=y_test,\n",
    ")\n",
    "\n",
    "accuracy_gbert = utilities.evaluate_model(\n",
    "    model=model_gbert,\n",
    "    x_test=data_test.batch(batch_size=batch_size),\n",
    "    y_test=y_test,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparative Analysis\n",
    "\n",
    "Performance was already quite high for the baseline model. This shows that a fairly simple model can already solve the task at hand pretty well, maybe because distinguishing between plain and regular language is not a particularly difficult task. The LSTM model performs similarly to the baseline model, suggesting that preserving word order does not improve model performance. Surprisingly, the model with pre-trained NNLM-embeddings performed even worse than the simple baseline model. The reasons for this bad performance are not quite clear. FInally, the fine-tuned version of German BERT outperformed all other models by a substantial margin. This high performance shows that the more advanced Transormer-based approach yields a superior text-classification accuracy, which is in line with existing research on the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_baseline = 0.876\n",
    "\n",
    "print(f\"Accuracy Baseline: {accuracy_baseline * 100:.1f}%\")\n",
    "print(f\"Accuracy LSTM:  {accuracy_lstm * 100:.1f}%\")\n",
    "print(f\"Accuracy NNLM: {accuracy_nnlm * 100:.1f}%\")\n",
    "print(f\"Accuracy German BERT: {accuracy_gbert * 100:.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
